"""
Small-Scale Tests for Stage 2 CodeT5p-770m Classifier

Verifies user requirements on 1000 AI samples:
1. Training works with checkpoints saved
2. Model is frozen (only head trained)
3. GPU is used optimally
"""

import numpy as np
import pandas as pd
import torch
from pathlib import Path
import sys
import tempfile

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.models.stage2_classifier import create_stage2_classifier


def load_small_dataset(n_samples=50):
    """Load small subset of training data for testing"""
    print("\n" + "="*70)
    print("Loading Small Test Dataset")
    print("="*70)

    # Load training data
    train_df = pd.read_parquet('data/train.parquet')

    # Filter AI samples only
    ai_df = train_df[train_df['label'] > 0]

    # Sample randomly
    if len(ai_df) > n_samples:
        ai_df = ai_df.sample(n=n_samples, random_state=42)

    print(f"Loaded {len(ai_df)} AI samples")

    # Extract features and labels
    X = np.random.randn(len(ai_df), 36)  # Dummy features (CodeT5 doesn't use them)
    y = ai_df['label'].values
    code_strings = ai_df['code'].tolist()

    # Split 80/20 train/test
    n_train = int(0.8 * len(X))
    X_train, X_test = X[:n_train], X[n_train:]
    y_train, y_test = y[:n_train], y[n_train:]
    code_train, code_test = code_strings[:n_train], code_strings[n_train:]

    print(f"  Train: {len(X_train)} samples")
    print(f"  Test: {len(X_test)} samples")
    print(f"  AI families: {np.unique(y)}")
    print("="*70)

    return X_train, X_test, y_train, y_test, code_train, code_test


def test_1_encoder_freezing():
    """Test 1: Verify encoder is frozen, head is trainable"""
    print("\n" + "="*70)
    print("TEST 1: Encoder Freezing Verification")
    print("="*70)

    clf = create_stage2_classifier('codet5', verbose=1)

    # Initialize model (downloads encoder)
    clf._initialize_model()

    # Check encoder parameters
    encoder_trainable = any(p.requires_grad for p in clf.encoder.parameters())
    head_trainable = any(p.requires_grad for p in clf.classification_head.parameters())

    print("\nParameter Status:")
    print(f"  Encoder requires_grad: {encoder_trainable}")
    print(f"  Head requires_grad: {head_trainable}")

    # Verify
    if encoder_trainable:
        print("\nâœ— FAIL: Encoder is trainable (should be frozen)!")
        return False
    if not head_trainable:
        print("\nâœ— FAIL: Head is not trainable (should be trainable)!")
        return False

    print("\nâœ“ PASS: Encoder frozen, head trainable")
    print("="*70)
    return True


def test_2_gpu_utilization():
    """Test 2: Check GPU memory and utilization"""
    print("\n" + "="*70)
    print("TEST 2: GPU Utilization Check")
    print("="*70)

    if not torch.cuda.is_available():
        print("âš  SKIP: CUDA not available")
        print("="*70)
        return True

    # Check GPU info
    print(f"GPU Device: {torch.cuda.get_device_name(0)}")
    print(f"Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")

    # Create model
    clf = create_stage2_classifier('codet5', batch_size=32, verbose=1)
    clf._initialize_model()

    # Check memory after model loading
    allocated = torch.cuda.memory_allocated(0) / 1024**3
    reserved = torch.cuda.memory_reserved(0) / 1024**3

    print(f"\nGPU Memory After Model Loading:")
    print(f"  Allocated: {allocated:.2f} GB")
    print(f"  Reserved: {reserved:.2f} GB")

    # Verify reasonable memory usage
    if allocated > 8.0:  # Should be ~3-4GB
        print(f"\nâš  WARNING: High memory usage ({allocated:.2f} GB)")
        print("  Expected: ~3-4 GB for encoder + head")
    else:
        print(f"\nâœ“ Memory usage within expected range")

    print("\nâœ“ PASS: GPU memory check complete")
    print("="*70)
    return True


def test_3_gradient_flow():
    """Test 3: Verify gradients only in classification head"""
    print("\n" + "="*70)
    print("TEST 3: Gradient Flow Verification")
    print("="*70)

    # Create tiny dataset
    X_tiny = np.random.randn(8, 36)
    y_tiny = np.array([1, 2, 3, 4, 5, 6, 7, 8])
    code_tiny = [f"def function_{i}():\n    return {i}" for i in range(8)]

    # Create and initialize model
    clf = create_stage2_classifier('codet5', batch_size=32, max_epochs=1, verbose=1)
    clf._initialize_model()

    # Forward pass
    embeddings = clf._encode_batch(code_tiny[:4])
    logits = clf.classification_head(embeddings)

    # Dummy loss
    criterion = torch.nn.CrossEntropyLoss()
    targets = torch.LongTensor([0, 1, 2, 3]).to(clf.device)
    loss = criterion(logits, targets)

    # Backward pass
    loss.backward()

    # Check gradients
    encoder_has_grads = any(p.grad is not None and torch.any(p.grad != 0) for p in clf.encoder.parameters())
    head_has_grads = any(p.grad is not None and torch.any(p.grad != 0) for p in clf.classification_head.parameters())

    print(f"\nGradient Status:")
    print(f"  Encoder has gradients: {encoder_has_grads}")
    print(f"  Head has gradients: {head_has_grads}")

    # Verify
    if encoder_has_grads:
        print("\nâœ— FAIL: Encoder has gradients (should be frozen)!")
        return False
    if not head_has_grads:
        print("\nâœ— FAIL: Head has no gradients (should receive gradients)!")
        return False

    print("\nâœ“ PASS: Gradients only in classification head")
    print("="*70)
    return True


def test_4_checkpoint_save_load():
    """Test 4: Verify checkpoint save/load integrity"""
    print("\n" + "="*70)
    print("TEST 4: Checkpoint Save/Load Verification")
    print("="*70)

    # Load small dataset (reduced for speed)
    X_train, X_test, y_train, y_test, code_train, code_test = load_small_dataset(n_samples=50)

    # Train model briefly
    print("\nTraining model on 200 samples (1 epoch)...")
    clf = create_stage2_classifier('codet5', batch_size=32, max_epochs=1, early_stopping=False, verbose=1)
    clf.fit(X_train, y_train, code_strings=code_train)

    # Get predictions before save
    preds_before = clf.predict(X_test, code_strings=code_test)
    print(f"\nPredictions before save: {preds_before[:10]}")

    # Save checkpoint
    with tempfile.TemporaryDirectory() as tmpdir:
        checkpoint_path = Path(tmpdir) / 'test_checkpoint.pth'
        clf.save(str(checkpoint_path))

        # Verify file exists and size
        assert checkpoint_path.exists(), "Checkpoint file not created!"
        file_size_mb = checkpoint_path.stat().st_size / (1024 * 1024)
        print(f"Checkpoint size: {file_size_mb:.2f} MB")

        if file_size_mb > 5.0:
            print(f"âš  WARNING: Large checkpoint ({file_size_mb:.2f} MB)")
            print("  Expected: ~1 MB (head only)")

        # Load checkpoint in new instance
        print("\nLoading checkpoint in new model instance...")
        clf_new = create_stage2_classifier('codet5', verbose=1)
        clf_new.load(str(checkpoint_path))

        # Get predictions after load
        preds_after = clf_new.predict(X_test, code_strings=code_test)
        print(f"Predictions after load: {preds_after[:10]}")

        # Verify predictions match
        if not np.array_equal(preds_before, preds_after):
            print("\nâœ— FAIL: Predictions differ after load!")
            print(f"  Differences: {np.sum(preds_before != preds_after)} / {len(preds_before)}")
            return False

    print("\nâœ“ PASS: Checkpoint integrity verified")
    print("="*70)
    return True


def run_all_tests():
    """Run all 4 verification tests"""
    print("\n" + "="*70)
    print("STAGE 2 CODET5P-770M SMALL-SCALE TESTS")
    print("="*70)
    print("\nVerifying user requirements:")
    print("  1. Training works with checkpoints saved")
    print("  2. Model is frozen (only head trained)")
    print("  3. GPU is used optimally")
    print("="*70)

    results = {}

    try:
        # Test 1: Encoder freezing
        results['test_1_freezing'] = test_1_encoder_freezing()

        # Test 2: GPU utilization
        results['test_2_gpu'] = test_2_gpu_utilization()

        # Test 3: Gradient flow
        results['test_3_gradients'] = test_3_gradient_flow()

        # Test 4: Checkpoint integrity
        results['test_4_checkpoint'] = test_4_checkpoint_save_load()

    except Exception as e:
        print(f"\nâœ— EXCEPTION: {e}")
        import traceback
        traceback.print_exc()
        results['exception'] = str(e)
        return False

    # Summary
    print("\n" + "="*70)
    print("TEST SUMMARY")
    print("="*70)

    all_passed = True
    for test_name, passed in results.items():
        status = "âœ“ PASS" if passed else "âœ— FAIL"
        print(f"{status}: {test_name}")
        if not passed:
            all_passed = False

    print("="*70)

    if all_passed:
        print("\nðŸŽ‰ ALL TESTS PASSED!")
        print("\nNext steps:")
        print("  1. Proceed to Phase 3: Full training on 58K AI samples")
        print("  2. Expected training time: 3-4 hours")
        print("  3. Command: uv run python src/training/train_stage2.py --classifier-type codet5")
        print("="*70)
        return True
    else:
        print("\nâš  SOME TESTS FAILED")
        print("Please fix issues before proceeding to full training.")
        print("="*70)
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
